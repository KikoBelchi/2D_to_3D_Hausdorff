{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"Visualization of the transforms in data_loading.py\"\"\"\n",
    "# **Author**: `Francisco Belch√≠ <frbegu@gmail.com>, <https://github.com/KikoBelchi/2d_to_3d>`_\n",
    "\n",
    "###\n",
    "### Imports\n",
    "###\n",
    "from __future__ import print_function, division\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform # package 'scikit-image'\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import data_loading\n",
    "import functions_data_processing\n",
    "import functions_plot\n",
    "\n",
    "# Imports for plotting\n",
    "import matplotlib.pyplot as plt # Do not use when running on the server\n",
    "from mpl_toolkits.mplot3d import axes3d # Do not use when running on the server\n",
    "\n",
    "# Allow the interactive rotation of 3D scatter plots in jupyter notebook\n",
    "import sys    \n",
    "import os    \n",
    "file_name =  os.path.basename(sys.argv[0])\n",
    "#print(file_name == 'ipykernel_launcher.py') # This basicaly asks whether this file is a jupyter notebook?\n",
    "if __name__ == \"__main__\":\n",
    "    if file_name == 'ipykernel_launcher.py': # Run only in .ipynb, not in exported .py scripts\n",
    "        get_ipython().run_line_magic('matplotlib', 'notebook') # Equivalent to ''%matplotlib notebook', but it is also understood by .py scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--sequence-name SEQUENCE_NAME]\n",
      "                             [--dataset-number N] [--reordered-dataset N]\n",
      "                             [--num-selected-vertices N]\n",
      "                             [--submesh-num-vertices-vertical N]\n",
      "                             [--submesh-num-vertices-horizontal N]\n",
      "                             [--batch-size N] [--num-epochs N] [--lr LR]\n",
      "                             [--momentum M] [--gamma N] [--no-cuda] [--seed S]\n",
      "                             [--log-interval N] [--log-epochs N]\n",
      "                             [--num-workers N] [--resnet-version N]\n",
      "                             [--frozen-resnet N] [--hyperpar-option N]\n",
      "                             [--crop-centre-or-ROI N] [--camera-coordinates N]\n",
      "                             [--random-seed-to-choose-video-sequences N]\n",
      "                             [--random-seed-to-shuffle-training-frames N]\n",
      "                             [--random-seed-to-shuffle-validation-frames N]\n",
      "                             [--random-seed-to-shuffle-test-frames N]\n",
      "                             [--lengths-proportion-test LENGTHS_PROPORTION_TEST]\n",
      "                             [--normalization N] [--uv-normalization N]\n",
      "                             [--normalize-distance-adj-vertices N]\n",
      "                             [--verbose N] [--testing-eval N]\n",
      "                             [--hyperpar-opt N] [--gpu-device N]\n",
      "                             [--predict-uv-or-xyz PREDICT_UV_OR_XYZ]\n",
      "                             [--loss N]\n",
      "                             [--testing-loss-computations TESTING_LOSS_COMPUTATIONS]\n",
      "                             [--neighbour-dist NEIGHBOUR_DIST]\n",
      "                             [--neighb-dist-weight NEIGHB_DIST_WEIGHT]\n",
      "                             [--squared N] [--normals N]\n",
      "                             [--batch-size-to-show N]\n",
      "                             [--degrees-of-each-rotation DEGREES_OF_EACH_ROTATION]\n",
      "                             [--directory-prepend DIRECTORY_PREPEND]\n",
      "                             [--swap-axes N] [--train-or-val N]\n",
      "                             [--plot-or-GIF N]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1001/jupyter/kernel-b29c94fc-c3f2-4c6f-aa99-aa5e60aa5898.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# This script worked before changing the list of arguments of data_loading.vertices_Dataset() by the args produced by the parser.\n",
    "# Now al lthis should be adapted and run from a .py script in order to work.\n",
    "\n",
    "# In particular, we will need to add the following and then modify the args as appropriate in each cell\n",
    "import argparse\n",
    "import functions_data_processing\n",
    "args = functions_data_processing.parser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e33846c55a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Horizontal Flip (applied only to the 2D image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvertex_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtsfrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvertex_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'args'"
     ]
    }
   ],
   "source": [
    "# Horizontal Flip (applied only to the 2D image)\n",
    "if __name__ == '__main__':\n",
    "    vertex_dataset = data_loading.vertices_Dataset()\n",
    "    tsfrm = transforms.RandomHorizontalFlip()\n",
    "    sample = vertex_dataset[22]\n",
    "    # RGBA_to_RGB(sample['image'])\n",
    "\n",
    "    from PIL import Image\n",
    "    im = Image.open(sample['img_name'])\n",
    "    # im.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "    plt.title('Original image')\n",
    "    plt.show()\n",
    "\n",
    "    transformed_image = tsfrm(im)\n",
    "    # transformed_image.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(transformed_image)\n",
    "    plt.title('Transformed image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Resized Crop (applied only to the 2D image)\n",
    "if __name__ == '__main__':\n",
    "    vertex_dataset = data_loading.vertices_Dataset()\n",
    "    tsfrm = transforms.RandomResizedCrop(224)\n",
    "    sample = vertex_dataset[22]\n",
    "    # RGBA_to_RGB(sample['image'])\n",
    "\n",
    "    im = Image.open(sample['img_name'])\n",
    "    # im.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "    plt.title('Original image')\n",
    "    plt.show()\n",
    "\n",
    "    transformed_image = tsfrm(im)\n",
    "    # transformed_image.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(transformed_image)\n",
    "    plt.title('Transformed image')\n",
    "    plt.show()\n",
    "\n",
    "    # The following does not work, so I had to use PIL images\n",
    "    # transformed_image = tsfrm(Image.open(io.BytesIO(sample['img_name'])))\n",
    "    # transformed_image = tsfrm(RGBA_to_RGB(sample['image']))\n",
    "    # fig = plt.figure()\n",
    "    # ax = plt.subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize (without cropping) (applied only to the 2D image)\n",
    "if __name__ == '__main__':\n",
    "    vertex_dataset = data_loading.vertices_Dataset()\n",
    "    tsfrm = transforms.Resize(10)\n",
    "    sample = vertex_dataset[22]\n",
    "    # RGBA_to_RGB(sample['image'])\n",
    "\n",
    "    im = Image.open(sample['img_name'])\n",
    "    # im.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "    plt.title('Original image')\n",
    "    plt.show()\n",
    "\n",
    "    transformed_image = tsfrm(im)\n",
    "    # transformed_image.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(transformed_image)\n",
    "    plt.title('Transformed image')\n",
    "    plt.show()\n",
    "\n",
    "    # The following does not work, so I had to use PIL images\n",
    "    # transformed_image = tsfrm(Image.open(io.BytesIO(sample['img_name'])))\n",
    "    # transformed_image = tsfrm(RGBA_to_RGB(sample['image']))\n",
    "    # fig = plt.figure()\n",
    "    # ax = plt.subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize (without cropping) (applied only to the 2D image)\n",
    "if __name__ == '__main__':\n",
    "    vertex_dataset = data_loading.vertices_Dataset()\n",
    "    tsfrm = transforms.Resize((10, 20))\n",
    "    sample = vertex_dataset[22]\n",
    "    # RGBA_to_RGB(sample['image'])\n",
    "\n",
    "    im = Image.open(sample['img_name'])\n",
    "    # im.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "    plt.title('Original image')\n",
    "    plt.show()\n",
    "\n",
    "    transformed_image = tsfrm(im)\n",
    "    # transformed_image.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(transformed_image)\n",
    "    plt.title('Transformed image')\n",
    "    plt.show()\n",
    "\n",
    "    # The following does not work, so I had to use PIL images\n",
    "    # transformed_image = tsfrm(Image.open(io.BytesIO(sample['img_name'])))\n",
    "    # transformed_image = tsfrm(RGBA_to_RGB(sample['image']))\n",
    "    # fig = plt.figure()\n",
    "    # ax = plt.subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize (without cropping) (applied only to the 2D image)\n",
    "if __name__ == '__main__':\n",
    "    vertex_dataset = data_loading.vertices_Dataset()\n",
    "    tsfrm = transforms.Resize((20, 20))\n",
    "    sample = vertex_dataset[22]\n",
    "    # RGBA_to_RGB(sample['image'])\n",
    "\n",
    "    im = Image.open(sample['img_name'])\n",
    "    # im.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "    plt.title('Original image')\n",
    "    plt.show()\n",
    "\n",
    "    transformed_image = tsfrm(im)\n",
    "    # transformed_image.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(transformed_image)\n",
    "    plt.title('Transformed image')\n",
    "    plt.show()\n",
    "\n",
    "    # The following does not work, so I had to use PIL images\n",
    "    # transformed_image = tsfrm(Image.open(io.BytesIO(sample['img_name'])))\n",
    "    # transformed_image = tsfrm(RGBA_to_RGB(sample['image']))\n",
    "    # fig = plt.figure()\n",
    "    # ax = plt.subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize to ResNet needed size (without cropping) (applied only to the 2D image)\n",
    "if __name__ == '__main__':\n",
    "    vertex_dataset = data_loading.vertices_Dataset()\n",
    "    tsfrm = transforms.Resize((224, 224))\n",
    "    sample = vertex_dataset[22]\n",
    "    # RGBA_to_RGB(sample['image'])\n",
    "\n",
    "    im = Image.open(sample['img_name'])\n",
    "    # im.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "    plt.title('Original image')\n",
    "    plt.show()\n",
    "\n",
    "    transformed_image = tsfrm(im)\n",
    "    # transformed_image.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(transformed_image)\n",
    "    plt.title('Transformed image')\n",
    "    plt.show()\n",
    "\n",
    "    # The following does not work, so I had to use PIL images\n",
    "    # transformed_image = tsfrm(Image.open(io.BytesIO(sample['img_name'])))\n",
    "    # transformed_image = tsfrm(RGBA_to_RGB(sample['image']))\n",
    "    # fig = plt.figure()\n",
    "    # ax = plt.subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### CAVEAT: Since our images have high resolution and many background pixels, \n",
    "### some random crops don't even show the towel\n",
    "###\n",
    "if __name__ == '__main__':\n",
    "    vertex_dataset = data_loading.vertices_Dataset()\n",
    "    sample = vertex_dataset[22]\n",
    "    plt.figure()\n",
    "    image = sample['image']\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original image')\n",
    "    plt.show()\n",
    "\n",
    "    # Lucky example in which most of the towel shows:\n",
    "    plt.figure()\n",
    "    image = io.imread('resizedRandomCrop_goodLuckExample')\n",
    "    plt.imshow(image)\n",
    "    plt.title('Lucky example in which most of the towel shows')\n",
    "    plt.show()\n",
    "\n",
    "    # Example in which only some of the towel shows:\n",
    "    plt.figure()\n",
    "    image = io.imread('resizedRandomCrop_averageLuckExample')\n",
    "    plt.imshow(image)\n",
    "    plt.title('Example in which only some of the towel shows')\n",
    "    plt.show()\n",
    "\n",
    "    # Bad luck example in which none of the towel shows:\n",
    "    plt.figure()\n",
    "    image = io.imread('resizedRandomCrop_badLuckExample')\n",
    "    plt.imshow(image)\n",
    "    plt.title('Example in which none of the towel shows')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAVEAT: Permutation of axes by 'torchvision.transforms.ToTensor' \n",
    "'torchvision.transforms.ToTensor' <br>\n",
    "converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a \n",
    "torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "\n",
    "To convert it back to a tensor of shape (H x W x C) in the range [0, 255], and to remove the extra alpha channel we are carrying with out RGBA pictures, we create the following functions:\n",
    "data_loading.unnormalize_RGB_of_HWC_tensor\n",
    "data_loading.unnormalize_RGB_of_CHW_tensor\n",
    "data_loading.tensor_to_plot\n",
    "\n",
    "# CAVEAT: Permutation of axes by 'torchvision.transforms.ToTensor'. Part II \n",
    "Notice, though, that 'torchvision.utils.make_grid' \n",
    "and some functions for CNNs take as input torch.FloatTensor of shape (C x H x W).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Composition of off-the-shelf torchvision.transforms\n",
    "###\n",
    "# Plot using 'data_loading.tensor_to_plot'\n",
    "if __name__ == '__main__':\n",
    "    vertex_dataset = data_loading.vertices_Dataset()\n",
    "\n",
    "    mean_for_normalization = [0.485, 0.456, 0.406]\n",
    "    std_for_normalization = [0.229, 0.224, 0.225]\n",
    "\n",
    "    tsfrm = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean_for_normalization, std_for_normalization)\n",
    "        ])\n",
    "\n",
    "    sample = vertex_dataset[22]\n",
    "    im = Image.open(sample['img_name'])\n",
    "    # print(im.size)\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "    plt.title('Original image')\n",
    "    plt.show()\n",
    "\n",
    "    transformed_image = tsfrm(im)\n",
    "    # print(type(transformed_image))\n",
    "    # print(transformed_image.shape)\n",
    "\n",
    "    transformed_image_toPlot = data_loading.tensor_to_plot(transformed_image, mean_for_normalization, std_for_normalization)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(transformed_image_toPlot) # Plot the R channel\n",
    "    plt.title('Transformed image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### All transforms together in the instanciation of the dataset class\n",
    "###\n",
    "# \n",
    "# Let's put this all together to create a dataset with composed\n",
    "# transforms.\n",
    "# To summarize, every time this dataset is sampled:\n",
    "# \n",
    "# -  An image is read from the file on the fly\n",
    "# -  Transforms are applied on the read image\n",
    "# -  Since some of the transforms are random, data is augmentated on\n",
    "#    sampling\n",
    "# \n",
    "# ##  Iterating through the dataset\n",
    "# \n",
    "# We can iterate over the created dataset with a loop as before,\n",
    "# using the function functions_plot.plot_a_fixed_list_images_randomly_transformed(transformed_dataset).\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.RandomResizedCrop(224),\n",
    "                                               transforms.RandomHorizontalFlip(),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]))\n",
    "\n",
    "    functions_plot.plot_a_fixed_list_images_randomly_transformed(transformed_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectangular box containing the towel in the 2D image rescaled to 224x224\n",
    "if __name__ == '__main__':\n",
    "    transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                     [0.229, 0.224, 0.225])\n",
    "                               ])  \n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform=transform,\n",
    "                                                   crop_centre_or_ROI=2)\n",
    "    functions_plot.plot_a_fixed_list_images_randomly_transformed(transformed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Only CenterCrop transform\n",
    "###\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.CenterCrop(224),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]))\n",
    "\n",
    "    functions_plot.plot_a_fixed_list_images_randomly_transformed(transformed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Using torch.utils.data.DataLoader\n",
    "###\n",
    "# \n",
    "# However, we are losing a lot of features by using a simple loop to\n",
    "# iterate over the data. In particular, we are missing out on:\n",
    "# \n",
    "# -  Batching the data\n",
    "# -  Shuffling the data\n",
    "# -  Load the data in parallel using ``multiprocessing`` workers.\n",
    "# \n",
    "# ``torch.utils.data.DataLoader`` is an iterator which provides all these\n",
    "# features. Parameters used below should be clear. One parameter of\n",
    "# interest is ``collate_fn``. You can specify how exactly the samples need\n",
    "# to be batched using ``collate_fn``. However, default collate should work\n",
    "# fine for most use cases.\n",
    "\n",
    "# Piece of code from when the transformed sample['image'] had the shape HxWxC instead of CxHxW:\n",
    "#     for i in range(batch_size):\n",
    "#         plt.figure()\n",
    "#         plt.imshow(images_batch[i, :, :, :]) \n",
    "# #         group_number, animation_frame = data_loading.group_and_frame_from_idx(i)\n",
    "# #         plt.title('Transformed image. Group no.: ' + str(group_number) + '. Frame = ' + str(animation_frame))\n",
    "#         plt.show()\n",
    "\n",
    "# Show a batch from DataLoader - only CenterCrop\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.CenterCrop(224),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]))\n",
    "\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=16,\n",
    "                            shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print(i_batch, sample_batched['image'].size(),\n",
    "              sample_batched['Vertex_coordinates'].size())\n",
    "\n",
    "        # observe 16th batch and stop.\n",
    "        if i_batch == 15:\n",
    "            plt.figure()\n",
    "            functions_plot.show_image_batch(sample_batched)\n",
    "            plt.axis('off')\n",
    "            plt.title('A random batch of unnormalized centre cropped images')\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "# Show a batch from DataLoader\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.RandomResizedCrop(224),\n",
    "                                               transforms.RandomHorizontalFlip(),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]))\n",
    "\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=5,\n",
    "                            shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print(i_batch, sample_batched['image'].size(),\n",
    "              sample_batched['Vertex_coordinates'].size())\n",
    "\n",
    "        # observe 5th batch and stop.\n",
    "        if i_batch == 4:\n",
    "            plt.figure()\n",
    "            functions_plot.show_image_batch(sample_batched)\n",
    "            plt.axis('off')\n",
    "            plt.title('A random batch of unnormalized transformed images')\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resize instead of crop\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.Resize(35),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]))\n",
    "\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=2,\n",
    "                            shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print(i_batch, sample_batched['image'].size(),\n",
    "              sample_batched['Vertex_coordinates'].size())\n",
    "\n",
    "        # observe 5th batch and stop.\n",
    "        if i_batch == 4:\n",
    "            plt.figure()\n",
    "            functions_plot.show_image_batch(sample_batched)\n",
    "            plt.axis('off')\n",
    "            plt.title('A batch of unnormalized Resized images')\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            break\n",
    "            \n",
    "### No resize nor crop\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]))\n",
    "\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=2,\n",
    "                            shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print(i_batch, sample_batched['image'].size(),\n",
    "              sample_batched['Vertex_coordinates'].size())\n",
    "\n",
    "        # observe 5th batch and stop.\n",
    "        if i_batch == 4:\n",
    "            plt.figure()\n",
    "            functions_plot.show_image_batch(sample_batched)\n",
    "            plt.axis('off')\n",
    "            plt.title('A batch of unnormalized images')\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resize ROI to 35 (very small to see the difference)\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.Resize(35),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]), crop_centre_or_ROI=1\n",
    "                                                       )\n",
    "\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=1,\n",
    "                            shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print(i_batch, sample_batched['image'].size(),\n",
    "              sample_batched['Vertex_coordinates'].size())\n",
    "\n",
    "        # observe 5th batch and stop.\n",
    "        if i_batch == 4:\n",
    "            plt.figure()\n",
    "            functions_plot.show_image_batch(sample_batched)\n",
    "            plt.axis('off')\n",
    "            plt.title('A batch of unnormalized Resized images')\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            break\n",
    "            \n",
    "### No resize of the ROI\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]), crop_centre_or_ROI=1\n",
    "                                                       )\n",
    "\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=1,\n",
    "                            shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print(i_batch, sample_batched['image'].size(),\n",
    "              sample_batched['Vertex_coordinates'].size())\n",
    "\n",
    "        # observe 5th batch and stop.\n",
    "        if i_batch == 4:\n",
    "            plt.figure()\n",
    "            functions_plot.show_image_batch(sample_batched)\n",
    "            plt.axis('off')\n",
    "            plt.title('A batch of unnormalized images')\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resize ROI to 224 (the amount needed for ResNet)\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.Resize(224),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]), crop_centre_or_ROI=1\n",
    "                                                       )\n",
    "\n",
    "    batch_size = 1\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                            shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print('Member of the batch number ' + str(i_batch) + '. Image size: ' + str(sample_batched['image'].size()))\n",
    "\n",
    "        # observe last member of the batch and stop.\n",
    "        if i_batch == batch_size-1:\n",
    "            plt.figure()\n",
    "            functions_plot.show_image_batch(sample_batched)\n",
    "            plt.axis('off')\n",
    "            plt.title('A batch of unnormalized Resized images')\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            break\n",
    "            \n",
    "### No resize of the ROI\n",
    "if __name__ == '__main__':\n",
    "    transformed_dataset = data_loading.vertices_Dataset(transform = \n",
    "                                           transforms.Compose([\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                           ]), crop_centre_or_ROI=1\n",
    "                                                       )\n",
    "\n",
    "    batch_size = 1\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                            shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print('Member of the batch number ' + str(i_batch) + '. Image size: ' + str(sample_batched['image'].size()))\n",
    "\n",
    "        # observe last member of the batch and stop.\n",
    "        if i_batch == batch_size-1:\n",
    "            plt.figure()\n",
    "            functions_plot.show_image_batch(sample_batched)\n",
    "            plt.axis('off')\n",
    "            plt.title('A batch of unnormalized images')\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2d_to_3d_ipynb",
   "language": "python",
   "name": "2d_to_3d_ipynb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
